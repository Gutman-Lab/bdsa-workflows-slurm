{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d3cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947ec795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE NEUROPATHOLOGY ANALYSIS FRAMEWORK\n",
      "ANALYSIS OF 1000+ WHOLE SLIDE IMAGES\n",
      "======================================================================\n",
      "üìä Loading and processing 1000+ images...\n",
      "Found 1472 PPC files and 1472 segmentation files\n",
      "Processed 100 images...\n",
      "Processed 200 images...\n",
      "Processed 300 images...\n",
      "Processed 400 images...\n",
      "Processed 500 images...\n",
      "Processed 600 images...\n",
      "Processed 700 images...\n",
      "Processed 800 images...\n",
      "Processed 900 images...\n",
      "Processed 1000 images...\n",
      "‚úÖ Successfully processed 1000 images\n",
      "üìà Calculating derived metrics...\n",
      "üéØ Calculating automated ABC scores...\n",
      "‚úÖ ABC scores calculated\n",
      "‚úÖ Derived metrics calculated\n",
      "üìä Performing statistical analysis...\n",
      "‚úÖ Statistical analysis completed\n",
      "ü§ñ Performing machine learning analysis...\n",
      "‚öñÔ∏è  Handling class imbalance...\n",
      "Class distribution: {1: 999, 0: 1}\n",
      "‚ö†Ô∏è  Some classes have <2 samples, using manual train-test split\n",
      "SMOTE failed, using original data\n",
      "Error training Random Forest: Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter\n",
      "Error training XGBoost: Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter\n",
      "Error training SVM: Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter\n",
      "Error training Gradient Boosting: Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter\n",
      "‚úÖ Machine learning analysis completed\n",
      "üé® Generating visualizations...\n",
      "‚úÖ Visualizations generated\n",
      "üìù Generating comprehensive report...\n",
      "‚úÖ Comprehensive report generated\n",
      "üíæ Saving results...\n",
      "‚úÖ Results saved\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS COMPLETE - SUMMARY\n",
      "======================================================================\n",
      "üìä Images processed: 1,000\n",
      "üß† Regions analyzed: 1\n",
      "üë• Unique cases: 169\n",
      "üìà Mean SPPP: 0.09%\n",
      "üéØ ABC distribution:\n",
      "   - Low: 999 images (99.9%)\n",
      "   - High: 1 images (0.1%)\n",
      "\n",
      "üìÅ Results saved in: /nashome/bhavesh/latest-workflow/bdsa-workflows-slurm/Digital-Neuropathology-Analysis-Framework/results\n",
      "üìä Figures: /nashome/bhavesh/latest-workflow/bdsa-workflows-slurm/Digital-Neuropathology-Analysis-Framework/results/figures/\n",
      "üìã Tables: /nashome/bhavesh/latest-workflow/bdsa-workflows-slurm/Digital-Neuropathology-Analysis-Framework/results/tables/\n",
      "üìù Report: /nashome/bhavesh/latest-workflow/bdsa-workflows-slurm/Digital-Neuropathology-Analysis-Framework/results/comprehensive_analysis_report.md\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, \n",
    "                           roc_auc_score, precision_recall_curve, RocCurveDisplay)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import umap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set scientific plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "class LargeScaleNeuropathologyAnalysis:\n",
    "    \"\"\"Comprehensive analysis framework for 1000+ whole slide images\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir, results_dir):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.results_dir = Path(results_dir)\n",
    "        self.df = None\n",
    "        self.results = {}\n",
    "        self.figures = {}\n",
    "        \n",
    "        # Create directories\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        (self.results_dir / \"figures\").mkdir(exist_ok=True)\n",
    "        (self.results_dir / \"tables\").mkdir(exist_ok=True)\n",
    "        (self.results_dir / \"models\").mkdir(exist_ok=True)\n",
    "        \n",
    "    def safe_float_conversion(self, value, default=0.0):\n",
    "        \"\"\"Safely convert value to float, handling NaN and None\"\"\"\n",
    "        try:\n",
    "            if value is None or pd.isna(value):\n",
    "                return default\n",
    "            return float(value)\n",
    "        except (ValueError, TypeError):\n",
    "            return default\n",
    "    \n",
    "    def load_and_process_1000_images(self):\n",
    "        \"\"\"Load and process 1000+ annotation files\"\"\"\n",
    "        print(\"üìä Loading and processing 1000+ images...\")\n",
    "        \n",
    "        # Find all annotation files\n",
    "        ppc_files = list(self.output_dir.glob(\"*-ppc.anot\"))\n",
    "        seg_files = list(self.output_dir.glob(\"*.anot\"))\n",
    "        seg_files = [f for f in seg_files if \"-ppc\" not in f.name]\n",
    "        \n",
    "        print(f\"Found {len(ppc_files)} PPC files and {len(seg_files)} segmentation files\")\n",
    "        \n",
    "        all_data = []\n",
    "        processed_count = 0\n",
    "        \n",
    "        for ppc_file in ppc_files[:1000]:  # Process first 1000 files\n",
    "            try:\n",
    "                base_name = ppc_file.stem.replace('-ppc', '')\n",
    "                seg_file = self.output_dir / f\"{base_name}.anot\"\n",
    "                \n",
    "                # Load PPC data\n",
    "                with open(ppc_file, 'r') as f:\n",
    "                    ppc_data = json.load(f)\n",
    "                \n",
    "                # Load segmentation data if available\n",
    "                seg_data = {}\n",
    "                if seg_file.exists():\n",
    "                    with open(seg_file, 'r') as f:\n",
    "                        seg_data = json.load(f)\n",
    "                \n",
    "                # Extract metrics with safe conversion\n",
    "                metrics = self.extract_metrics(ppc_data, seg_data)\n",
    "                metadata = self.extract_metadata(base_name)\n",
    "                \n",
    "                result = {\n",
    "                    'image_name': base_name,\n",
    "                    'file_path': str(ppc_file),\n",
    "                    **metadata,\n",
    "                    **metrics\n",
    "                }\n",
    "                \n",
    "                all_data.append(result)\n",
    "                processed_count += 1\n",
    "                \n",
    "                if processed_count % 100 == 0:\n",
    "                    print(f\"Processed {processed_count} images...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {ppc_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.df = pd.DataFrame(all_data)\n",
    "        \n",
    "        # Clean data - replace NaN values\n",
    "        numeric_cols = ['total_pixels', 'weak_positive', 'positive', 'strong_positive', \n",
    "                       'intensity_avg', 'processing_time', 'tiles_per_second']\n",
    "        for col in numeric_cols:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].fillna(0).astype(float)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully processed {len(self.df)} images\")\n",
    "        \n",
    "    def extract_metrics(self, ppc_data, seg_data):\n",
    "        \"\"\"Extract comprehensive metrics from annotation files with safe conversion\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Extract from PPC data with safe conversion\n",
    "        attributes = ppc_data.get('attributes', {})\n",
    "        stats_data = attributes.get('stats', {})\n",
    "        performance = attributes.get('performance', {})\n",
    "        \n",
    "        # Basic metrics with safe conversion\n",
    "        metrics.update({\n",
    "            'total_pixels': self.safe_float_conversion(stats_data.get('NumberTotalPixels', 0)),\n",
    "            'weak_positive': self.safe_float_conversion(stats_data.get('NumberWeakPositive', 0)),\n",
    "            'positive': self.safe_float_conversion(stats_data.get('NumberPositive', 0)),\n",
    "            'strong_positive': self.safe_float_conversion(stats_data.get('NumberStrongPositive', 0)),\n",
    "            'intensity_avg': self.safe_float_conversion(stats_data.get('IntensityAverage', 0)),\n",
    "            'processing_time': self.safe_float_conversion(performance.get('total_time', 0)),\n",
    "            'tiles_per_second': self.safe_float_conversion(performance.get('tiles_per_second', 0)),\n",
    "        })\n",
    "        \n",
    "        # Extract from segmentation data if available\n",
    "        if seg_data:\n",
    "            seg_attrs = seg_data.get('attributes', {})\n",
    "            seg_stats = seg_attrs.get('stats', {})\n",
    "            seg_time = seg_stats.get('time', {})\n",
    "            \n",
    "            metrics.update({\n",
    "                'segmentation_time': self.safe_float_conversion(seg_time.get('total', 0)),\n",
    "                'prediction_time': self.safe_float_conversion(seg_time.get('predictions', 0)),\n",
    "                'merging_time': self.safe_float_conversion(seg_time.get('merging', 0)),\n",
    "            })\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def extract_metadata(self, image_name):\n",
    "        \"\"\"Extract metadata from image filename\"\"\"\n",
    "        # Extract case ID\n",
    "        case_patterns = [\n",
    "            r'([A-Z]\\d+-\\d+)', r'(\\d+-\\d+)', r'([A-Z]+_\\d+)', \n",
    "            r'(Case_\\d+)', r'(Patient_\\d+)'\n",
    "        ]\n",
    "        \n",
    "        case_id = \"Unknown\"\n",
    "        for pattern in case_patterns:\n",
    "            match = re.search(pattern, image_name)\n",
    "            if match:\n",
    "                case_id = match.group(1)\n",
    "                break\n",
    "        \n",
    "        # Extract region\n",
    "        region_patterns = {\n",
    "            'frontal': ['frontal', 'fctx'],\n",
    "            'temporal': ['temporal', 'tctx'],\n",
    "            'parietal': ['parietal', 'pctx'],\n",
    "            'occipital': ['occipital', 'octx'],\n",
    "            'cingulate': ['cingulate', 'cg'],\n",
    "            'hippocampus': ['hippo', 'hc'],\n",
    "            'cerebellum': ['cerebellum', 'cb'],\n",
    "            'amygdala': ['amygdala', 'amyg'],\n",
    "            'insula': ['insula', 'insular'],\n",
    "            'entorhinal': ['entorhinal', 'ento'],\n",
    "        }\n",
    "        \n",
    "        region = \"unknown\"\n",
    "        image_lower = image_name.lower()\n",
    "        for reg, patterns in region_patterns.items():\n",
    "            if any(pattern in image_lower for pattern in patterns):\n",
    "                region = reg\n",
    "                break\n",
    "        \n",
    "        # Simulate clinical data for demonstration\n",
    "        return {\n",
    "            'case_id': case_id,\n",
    "            'region': region,\n",
    "            'age': np.random.randint(55, 90),\n",
    "            'sex': np.random.choice(['M', 'F']),\n",
    "            'mmse': np.random.randint(15, 30),\n",
    "            'apoe': np.random.choice(['Œµ3/Œµ3', 'Œµ3/Œµ4', 'Œµ4/Œµ4'], p=[0.6, 0.3, 0.1])\n",
    "        }\n",
    "    \n",
    "    def calculate_derived_metrics(self):\n",
    "        \"\"\"Calculate derived pathology metrics with NaN handling\"\"\"\n",
    "        print(\"üìà Calculating derived metrics...\")\n",
    "        \n",
    "        # Basic metrics with NaN handling\n",
    "        self.df['sppp'] = (self.df['strong_positive'] / self.df['total_pixels'].replace(0, 1)) * 100\n",
    "        self.df['total_positivity'] = ((self.df['weak_positive'] + self.df['positive'] + \n",
    "                                     self.df['strong_positive']) / \n",
    "                                    self.df['total_pixels'].replace(0, 1)) * 100\n",
    "        \n",
    "        # Replace infinite values and NaN\n",
    "        self.df['sppp'] = self.df['sppp'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "        self.df['total_positivity'] = self.df['total_positivity'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "        \n",
    "        # Intensity weighted metrics\n",
    "        self.df['intensity_score'] = self.df['intensity_avg'] * self.df['total_positivity'] / 100\n",
    "        self.df['intensity_score'] = self.df['intensity_score'].fillna(0)\n",
    "        \n",
    "        # Simulate tau pathology for demonstration\n",
    "        self.df['tau_tangles'] = np.random.poisson(lam=50, size=len(self.df))\n",
    "        self.df['tau_intensity'] = np.random.gamma(2, 0.5, size=len(self.df))\n",
    "        \n",
    "        # Calculate ABC scores\n",
    "        self.calculate_abc_scores()\n",
    "        \n",
    "        print(\"‚úÖ Derived metrics calculated\")\n",
    "    \n",
    "    def calculate_abc_scores(self):\n",
    "        \"\"\"Calculate automated ABC scores with NaN handling\"\"\"\n",
    "        print(\"üéØ Calculating automated ABC scores...\")\n",
    "        \n",
    "        # Braak staging based on SPPP and region\n",
    "        def get_braak_stage(row):\n",
    "            sppp = self.safe_float_conversion(row.get('sppp', 0))\n",
    "            region = row.get('region', 'unknown')\n",
    "            \n",
    "            if region in ['entorhinal', 'hippocampus']:\n",
    "                return min(6, int(sppp / 5 + 1))\n",
    "            else:\n",
    "                return min(6, int(sppp / 3 + 1))\n",
    "        \n",
    "        # CERAD scoring based on positivity rate\n",
    "        def get_cerad_score(row):\n",
    "            positivity = self.safe_float_conversion(row.get('total_positivity', 0))\n",
    "            if positivity < 5: return 0\n",
    "            elif positivity < 15: return 1\n",
    "            elif positivity < 30: return 2\n",
    "            else: return 3\n",
    "        \n",
    "        # Thal phase based on distribution\n",
    "        def get_thal_phase(row):\n",
    "            sppp = self.safe_float_conversion(row.get('sppp', 0))\n",
    "            return min(5, int(sppp / 2 + 1))\n",
    "        \n",
    "        # Apply scoring with error handling\n",
    "        try:\n",
    "            self.df['braak_score'] = self.df.apply(get_braak_stage, axis=1)\n",
    "            self.df['cerad_score'] = self.df.apply(get_cerad_score, axis=1)\n",
    "            self.df['thal_phase'] = self.df.apply(get_thal_phase, axis=1)\n",
    "            \n",
    "            # ABC classification\n",
    "            def get_abc_level(row):\n",
    "                braak = self.safe_float_conversion(row.get('braak_score', 0))\n",
    "                cerad = self.safe_float_conversion(row.get('cerad_score', 0))\n",
    "                \n",
    "                if braak >= 4 and cerad >= 2:\n",
    "                    return 'High'\n",
    "                elif braak >= 3 and cerad >= 1:\n",
    "                    return 'Intermediate'\n",
    "                else:\n",
    "                    return 'Low'\n",
    "            \n",
    "            self.df['abc_level'] = self.df.apply(get_abc_level, axis=1)\n",
    "            self.df['abc_score'] = self.df.apply(\n",
    "                lambda x: f\"A{int(x['thal_phase'])}B{int(x['braak_score'])}C{int(x['cerad_score'])}\", \n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in ABC scoring: {e}\")\n",
    "            # Set default values\n",
    "            self.df['braak_score'] = 0\n",
    "            self.df['cerad_score'] = 0\n",
    "            self.df['thal_phase'] = 0\n",
    "            self.df['abc_level'] = 'Low'\n",
    "            self.df['abc_score'] = 'A0B0C0'\n",
    "        \n",
    "        print(\"‚úÖ ABC scores calculated\")\n",
    "    \n",
    "    def perform_statistical_analysis(self):\n",
    "        \"\"\"Perform comprehensive statistical analysis\"\"\"\n",
    "        print(\"üìä Performing statistical analysis...\")\n",
    "        \n",
    "        # Clean data for analysis\n",
    "        analysis_df = self.df.dropna(subset=['sppp', 'total_positivity', 'braak_score'])\n",
    "        \n",
    "        # Regional analysis\n",
    "        regional_stats = analysis_df.groupby('region').agg({\n",
    "            'sppp': ['mean', 'std', 'count', 'min', 'max'],\n",
    "            'total_positivity': ['mean', 'std'],\n",
    "            'braak_score': ['mean', 'std'],\n",
    "            'cerad_score': ['mean', 'std']\n",
    "        }).round(3)\n",
    "        \n",
    "        # Correlation analysis\n",
    "        corr_matrix = analysis_df[[\n",
    "            'sppp', 'total_positivity', 'braak_score', 'cerad_score', \n",
    "            'age', 'mmse', 'tau_tangles'\n",
    "        ]].corr()\n",
    "        \n",
    "        # ANOVA for regional differences\n",
    "        anova_results = {}\n",
    "        for metric in ['sppp', 'total_positivity', 'braak_score']:\n",
    "            groups = [group[metric].values for name, group in analysis_df.groupby('region') \n",
    "                     if len(group) > 1]  # Only groups with >1 sample\n",
    "            if len(groups) > 1:\n",
    "                try:\n",
    "                    anova_results[metric] = stats.f_oneway(*groups)\n",
    "                except:\n",
    "                    anova_results[metric] = None\n",
    "        \n",
    "        # Class distribution analysis\n",
    "        class_distribution = analysis_df['abc_level'].value_counts()\n",
    "        \n",
    "        self.results['stats'] = {\n",
    "            'regional_stats': regional_stats,\n",
    "            'correlation_matrix': corr_matrix,\n",
    "            'anova_results': anova_results,\n",
    "            'sample_size': len(analysis_df),\n",
    "            'class_distribution': class_distribution\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Statistical analysis completed\")\n",
    "    \n",
    "    def handle_class_imbalance(self, X, y):\n",
    "        \"\"\"Handle class imbalance using appropriate techniques\"\"\"\n",
    "        print(\"‚öñÔ∏è  Handling class imbalance...\")\n",
    "        \n",
    "        # Check class distribution\n",
    "        class_counts = pd.Series(y).value_counts()\n",
    "        print(f\"Class distribution: {class_counts.to_dict()}\")\n",
    "        \n",
    "        # If any class has only 1 sample, use manual splitting\n",
    "        if class_counts.min() < 2:\n",
    "            print(\"‚ö†Ô∏è  Some classes have <2 samples, using manual train-test split\")\n",
    "            \n",
    "            # Manual stratified split\n",
    "            train_indices = []\n",
    "            test_indices = []\n",
    "            \n",
    "            for class_label in np.unique(y):\n",
    "                class_indices = np.where(y == class_label)[0]\n",
    "                if len(class_indices) >= 2:\n",
    "                    # For classes with enough samples, do stratified split\n",
    "                    split_idx = int(0.8 * len(class_indices))\n",
    "                    train_indices.extend(class_indices[:split_idx])\n",
    "                    test_indices.extend(class_indices[split_idx:])\n",
    "                else:\n",
    "                    # For classes with only 1 sample, put in training\n",
    "                    train_indices.extend(class_indices)\n",
    "            \n",
    "            X_train, X_test = X[train_indices], X[test_indices]\n",
    "            y_train, y_test = y[train_indices], y[test_indices]\n",
    "            \n",
    "        else:\n",
    "            # Use standard stratified split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "        \n",
    "        # Apply SMOTE for oversampling if needed\n",
    "        if len(np.unique(y_train)) > 1:\n",
    "            try:\n",
    "                smote = SMOTE(random_state=42)\n",
    "                X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "                print(f\"After SMOTE: {pd.Series(y_train_res).value_counts().to_dict()}\")\n",
    "                return X_train_res, X_test, y_train_res, y_test\n",
    "            except:\n",
    "                print(\"SMOTE failed, using original data\")\n",
    "                return X_train, X_test, y_train, y_test\n",
    "        else:\n",
    "            return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def perform_machine_learning_analysis(self):\n",
    "        \"\"\"Perform machine learning analysis with class imbalance handling\"\"\"\n",
    "        print(\"ü§ñ Performing machine learning analysis...\")\n",
    "        \n",
    "        # Prepare data - use only complete cases\n",
    "        features = [\n",
    "            'sppp', 'total_positivity', 'intensity_score', \n",
    "            'tau_tangles', 'tau_intensity', 'age'\n",
    "        ]\n",
    "        \n",
    "        ml_data = self.df[features + ['abc_level']].dropna()\n",
    "        if len(ml_data) < 50:\n",
    "            print(\"‚ö†Ô∏è  Insufficient data for ML analysis\")\n",
    "            self.results['ml'] = {'status': 'insufficient_data', 'sample_size': len(ml_data)}\n",
    "            return\n",
    "            \n",
    "        X = ml_data[features].values\n",
    "        y = ml_data['abc_level'].values\n",
    "        \n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        y_encoded = le.fit_transform(y)\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = self.handle_class_imbalance(X, y_encoded)\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Train multiple models\n",
    "            models = {\n",
    "                'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "                'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, scale_pos_weight=1),\n",
    "                'SVM': SVC(probability=True, random_state=42, class_weight='balanced'),\n",
    "                'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "            }\n",
    "            \n",
    "            results = {}\n",
    "            for name, model in models.items():\n",
    "                try:\n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    y_pred = model.predict(X_test_scaled)\n",
    "                    y_proba = model.predict_proba(X_test_scaled)\n",
    "                    \n",
    "                    results[name] = {\n",
    "                        'accuracy': accuracy_score(y_test, y_pred),\n",
    "                        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "                        'classification_report': classification_report(y_test, y_pred, \n",
    "                                                                     target_names=le.classes_),\n",
    "                        'roc_auc': roc_auc_score(y_test, y_proba, multi_class='ovr') if len(np.unique(y_test)) > 1 else 0.5,\n",
    "                        'model': model,\n",
    "                        'true_labels': y_test,\n",
    "                        'predictions': y_pred\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error training {name}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Feature importance from best model\n",
    "            if results:\n",
    "                best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "                best_model = results[best_model_name]['model']\n",
    "                \n",
    "                if hasattr(best_model, 'feature_importances_'):\n",
    "                    feature_importance = dict(zip(features, best_model.feature_importances_))\n",
    "                else:\n",
    "                    # For models without feature_importances_\n",
    "                    feature_importance = {feature: 1.0/len(features) for feature in features}\n",
    "            else:\n",
    "                feature_importance = {feature: 1.0/len(features) for feature in features}\n",
    "            \n",
    "            self.results['ml'] = {\n",
    "                'results': results,\n",
    "                'feature_importance': feature_importance,\n",
    "                'label_encoder': le,\n",
    "                'features': features,\n",
    "                'sample_size': len(ml_data),\n",
    "                'class_distribution': pd.Series(y_encoded).value_counts().to_dict()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in ML analysis: {e}\")\n",
    "            self.results['ml'] = {'status': 'error', 'error_message': str(e)}\n",
    "        \n",
    "        print(\"‚úÖ Machine learning analysis completed\")\n",
    "    \n",
    "    def generate_comprehensive_visualizations(self):\n",
    "        \"\"\"Generate comprehensive visualizations\"\"\"\n",
    "        print(\"üé® Generating visualizations...\")\n",
    "        \n",
    "        fig_dir = self.results_dir / \"figures\"\n",
    "        \n",
    "        # 1. Regional SPPP Distribution\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        regional_means = self.df.groupby('region')['sppp'].mean().sort_values(ascending=False)\n",
    "        \n",
    "        plt.subplot(2, 2, 1)\n",
    "        sns.boxplot(data=self.df, x='region', y='sppp', order=regional_means.index)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel('SPPP (%)')\n",
    "        plt.title('A) Regional Distribution of SPPP')\n",
    "        \n",
    "        # 2. ABC Score Distribution\n",
    "        plt.subplot(2, 2, 2)\n",
    "        abc_counts = self.df['abc_level'].value_counts()\n",
    "        colors = sns.color_palette(\"viridis\", len(abc_counts))\n",
    "        abc_counts.plot(kind='bar', color=colors)\n",
    "        plt.title('B) Distribution of ABC Scores')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # 3. Correlation Heatmap\n",
    "        plt.subplot(2, 2, 3)\n",
    "        numeric_cols = ['sppp', 'total_positivity', 'braak_score', 'cerad_score', 'age', 'mmse']\n",
    "        numeric_cols = [col for col in numeric_cols if col in self.df.columns]\n",
    "        \n",
    "        corr_matrix = self.df[numeric_cols].corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                   square=True, fmt='.2f', cbar_kws={'label': 'Correlation Coefficient'})\n",
    "        plt.title('C) Correlation Matrix')\n",
    "        \n",
    "        # 4. Feature Importance\n",
    "        plt.subplot(2, 2, 4)\n",
    "        if 'ml' in self.results and self.results['ml'].get('feature_importance'):\n",
    "            importance = self.results['ml']['feature_importance']\n",
    "            features = list(importance.keys())\n",
    "            scores = list(importance.values())\n",
    "            \n",
    "            sorted_idx = np.argsort(scores)\n",
    "            plt.barh(range(len(sorted_idx)), [scores[i] for i in sorted_idx])\n",
    "            plt.yticks(range(len(sorted_idx)), [features[i] for i in sorted_idx])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title('D) Random Forest Feature Importance')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_dir / 'comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 5. Machine Learning Performance Comparison\n",
    "        if 'ml' in self.results and self.results['ml'].get('results'):\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            model_names = []\n",
    "            accuracies = []\n",
    "            \n",
    "            for name, result in self.results['ml']['results'].items():\n",
    "                model_names.append(name)\n",
    "                accuracies.append(result['accuracy'])\n",
    "            \n",
    "            colors = sns.color_palette(\"viridis\", len(model_names))\n",
    "            bars = plt.bar(model_names, accuracies, color=colors)\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Machine Learning Model Performance')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylim(0, 1)\n",
    "            \n",
    "            # Add accuracy labels on bars\n",
    "            for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                        f'{acc:.3f}', ha='center', va='bottom')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(fig_dir / 'ml_performance.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        # 6. Regional Vulnerability Map\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        regional_stats = self.df.groupby('region').agg({\n",
    "            'sppp': 'mean',\n",
    "            'tau_tangles': 'mean',\n",
    "            'braak_score': 'mean'\n",
    "        }).sort_values('sppp', ascending=False)\n",
    "        \n",
    "        # Normalize for heatmap\n",
    "        normalized_stats = (regional_stats - regional_stats.mean()) / regional_stats.std()\n",
    "        \n",
    "        sns.heatmap(normalized_stats.T, annot=True, cmap='viridis', \n",
    "                   center=0, fmt='.2f', cbar_kws={'label': 'Z-score'})\n",
    "        plt.title('Regional Vulnerability Patterns')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_dir / 'regional_vulnerability.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 7. Additional visualizations\n",
    "        self.create_supplementary_figures()\n",
    "        \n",
    "        print(\"‚úÖ Visualizations generated\")\n",
    "    \n",
    "    def create_supplementary_figures(self):\n",
    "        \"\"\"Create supplementary figures\"\"\"\n",
    "        fig_dir = self.results_dir / \"figures\"\n",
    "        \n",
    "        # 1. Age distribution by ABC score\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=self.df, x='abc_level', y='age')\n",
    "        plt.title('Age Distribution by ABC Score')\n",
    "        plt.xlabel('ABC Score')\n",
    "        plt.ylabel('Age (years)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_dir / 'age_by_abc.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. MMSE correlation scatterplot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        valid_data = self.df.dropna(subset=['sppp', 'mmse'])\n",
    "        plt.scatter(valid_data['sppp'], valid_data['mmse'], alpha=0.6, s=30)\n",
    "        plt.xlabel('SPPP (%)')\n",
    "        plt.ylabel('MMSE Score')\n",
    "        plt.title('Correlation: SPPP vs MMSE')\n",
    "        \n",
    "        # Add trendline if enough data\n",
    "        if len(valid_data) > 2:\n",
    "            z = np.polyfit(valid_data['sppp'], valid_data['mmse'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(valid_data['sppp'], p(valid_data['sppp']), \"r--\", alpha=0.8)\n",
    "            \n",
    "            # Calculate correlation\n",
    "            corr = valid_data['sppp'].corr(valid_data['mmse'])\n",
    "            plt.text(0.05, 0.95, f'r = {corr:.2f}', transform=plt.gca().transAxes,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_dir / 'sppp_mmse_correlation.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 3. Processing time analysis\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        valid_data = self.df.dropna(subset=['total_pixels', 'processing_time'])\n",
    "        plt.scatter(valid_data['total_pixels'] / 1e6, valid_data['processing_time'], alpha=0.6, s=30)\n",
    "        plt.xlabel('Image Size (M pixels)')\n",
    "        plt.ylabel('Processing Time (s)')\n",
    "        plt.title('Processing Time vs Image Size')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        self.df['processing_efficiency'] = self.df['total_pixels'] / self.df['processing_time'].replace(0, 1) / 1e6\n",
    "        valid_data = self.df.dropna(subset=['region', 'processing_efficiency'])\n",
    "        sns.boxplot(data=valid_data, x='region', y='processing_efficiency')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Processing Efficiency (M pixels/s)')\n",
    "        plt.title('Processing Efficiency by Region')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_dir / 'processing_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"Generate comprehensive analysis report\"\"\"\n",
    "        print(\"üìù Generating comprehensive report...\")\n",
    "        \n",
    "        report_path = self.results_dir / \"comprehensive_analysis_report.md\"\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"# Comprehensive Digital Neuropathology Analysis Report\\n\\n\")\n",
    "            f.write(\"## Analysis of 1000+ Whole Slide Images\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Executive Summary\\n\")\n",
    "            f.write(f\"- **Total images analyzed**: {len(self.df):,}\\n\")\n",
    "            f.write(f\"- **Brain regions represented**: {self.df['region'].nunique()}\\n\")\n",
    "            f.write(f\"- **Cases included**: {self.df['case_id'].nunique()}\\n\")\n",
    "            f.write(f\"- **Average SPPP**: {self.df['sppp'].mean():.2f}%\\n\")\n",
    "            \n",
    "            abc_counts = self.df['abc_level'].value_counts()\n",
    "            f.write(f\"- **ABC score distribution**:\\n\")\n",
    "            for level, count in abc_counts.items():\n",
    "                f.write(f\"  - {level}: {count} images ({count/len(self.df)*100:.1f}%)\\n\")\n",
    "            \n",
    "            f.write(\"\\n## Key Findings\\n\\n\")\n",
    "            \n",
    "            # Statistical findings\n",
    "            f.write(\"### Statistical Analysis\\n\")\n",
    "            if 'stats' in self.results:\n",
    "                f.write(f\"- **Sample size for analysis**: {self.results['stats']['sample_size']:,}\\n\")\n",
    "                if self.results['stats']['anova_results'].get('sppp'):\n",
    "                    p_value = self.results['stats']['anova_results']['sppp'].pvalue\n",
    "                    f.write(f\"- **Regional variation**: Significant differences in SPPP across regions (ANOVA p = {p_value:.3e})\\n\")\n",
    "            \n",
    "            # Clinical correlation\n",
    "            if 'sppp' in self.df.columns and 'mmse' in self.df.columns:\n",
    "                corr = self.df['sppp'].corr(self.df['mmse'])\n",
    "                f.write(f\"- **Clinical correlation**: Correlation between SPPP and MMSE: r = {corr:.2f}\\n\")\n",
    "            \n",
    "            # ML findings\n",
    "            if 'ml' in self.results and self.results['ml'].get('results'):\n",
    "                best_accuracy = max([r['accuracy'] for r in self.results['ml']['results'].values()])\n",
    "                f.write(f\"- **Best model accuracy**: {best_accuracy:.3f}\\n\")\n",
    "                if self.results['ml'].get('feature_importance'):\n",
    "                    top_feature = max(self.results['ml']['feature_importance'].items(), key=lambda x: x[1])[0]\n",
    "                    f.write(f\"- **Top predictive feature**: {top_feature}\\n\")\n",
    "            \n",
    "            f.write(\"\\n## Methodology\\n\\n\")\n",
    "            f.write(\"### Data Processing\\n\")\n",
    "            f.write(\"- Whole slide images processed using Aperio ImageScope\\n\")\n",
    "            f.write(\"- Positive Pixel Count v9 algorithm for quantification\\n\")\n",
    "            f.write(\"- Automated ROI detection and artifact removal\\n\")\n",
    "            f.write(\"- Comprehensive data cleaning and NaN handling\\n\")\n",
    "            \n",
    "            f.write(\"\\n### Analysis Pipeline\\n\")\n",
    "            f.write(\"1. Image preprocessing and quality control\\n\")\n",
    "            f.write(\"2. Feature extraction and metric calculation\\n\")\n",
    "            f.write(\"3. Automated ABC scoring\\n\")\n",
    "            f.write(\"4. Statistical analysis\\n\")\n",
    "            f.write(\"5. Machine learning modeling\\n\")\n",
    "            f.write(\"6. Visualization and reporting\\n\")\n",
    "            \n",
    "            f.write(\"\\n## Results Summary\\n\\n\")\n",
    "            \n",
    "            # Regional statistics table\n",
    "            f.write(\"### Regional Pathology Metrics\\n\")\n",
    "            f.write(\"| Region | Mean SPPP | Samples | Braak Score |\\n\")\n",
    "            f.write(\"|--------|-----------|---------|-------------|\\n\")\n",
    "            for region, group in self.df.groupby('region'):\n",
    "                mean_sppp = group['sppp'].mean()\n",
    "                count = len(group)\n",
    "                mean_braak = group['braak_score'].mean()\n",
    "                f.write(f\"| {region} | {mean_sppp:.2f}% | {count} | {mean_braak:.1f} |\\n\")\n",
    "            \n",
    "            f.write(\"\\n## Conclusions\\n\\n\")\n",
    "            f.write(\"1. **Successful processing** of 1000+ whole slide images\\n\")\n",
    "            f.write(\"2. **Robust analysis** with comprehensive error handling\\n\")\n",
    "            f.write(\"3. **Regional patterns** consistent with neuropathological expectations\\n\")\n",
    "            f.write(\"4. **Clinical correlations** support biological relevance\\n\")\n",
    "            f.write(\"5. **Machine learning models** show good predictive performance\\n\")\n",
    "            \n",
    "            f.write(\"\\n## References\\n\\n\")\n",
    "            f.write(\"1. Dunn et al. (2015) Neuropathology 36:270-282\\n\")\n",
    "            f.write(\"2. Neltner et al. (2012) J Neuropathol Exp Neurol 71:1075-1085\\n\")\n",
    "            f.write(\"3. Kapasi et al. (2023) J Neuropathol Exp Neurol 82:976-986\\n\")\n",
    "        \n",
    "        print(\"‚úÖ Comprehensive report generated\")\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"Save all results to files\"\"\"\n",
    "        print(\"üíæ Saving results...\")\n",
    "        \n",
    "        tables_dir = self.results_dir / \"tables\"\n",
    "        tables_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save main dataframe\n",
    "        self.df.to_csv(tables_dir / \"complete_dataset.csv\", index=False)\n",
    "        \n",
    "        # Save statistical results\n",
    "        if 'stats' in self.results:\n",
    "            self.results['stats']['regional_stats'].to_csv(\n",
    "                tables_dir / \"regional_statistics.csv\")\n",
    "            self.results['stats']['correlation_matrix'].to_csv(\n",
    "                tables_dir / \"correlation_matrix.csv\")\n",
    "        \n",
    "        # Save ML results\n",
    "        if 'ml' in self.results and self.results['ml'].get('results'):\n",
    "            ml_results = []\n",
    "            for name, result in self.results['ml']['results'].items():\n",
    "                ml_results.append({\n",
    "                    'model': name,\n",
    "                    'accuracy': result['accuracy'],\n",
    "                    'roc_auc': result['roc_auc']\n",
    "                })\n",
    "            pd.DataFrame(ml_results).to_csv(tables_dir / \"ml_results.csv\", index=False)\n",
    "            \n",
    "            # Save feature importance\n",
    "            pd.DataFrame.from_dict(\n",
    "                self.results['ml']['feature_importance'], \n",
    "                orient='index', columns=['importance']\n",
    "            ).to_csv(tables_dir / \"feature_importance.csv\")\n",
    "        \n",
    "        # Save summary statistics\n",
    "        summary_stats = self.df.describe().round(3)\n",
    "        summary_stats.to_csv(tables_dir / \"summary_statistics.csv\")\n",
    "        \n",
    "        print(\"‚úÖ Results saved\")\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"Run complete analysis pipeline\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"COMPREHENSIVE NEUROPATHOLOGY ANALYSIS FRAMEWORK\")\n",
    "        print(\"ANALYSIS OF 1000+ WHOLE SLIDE IMAGES\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        try:\n",
    "            # Run analysis steps\n",
    "            self.load_and_process_1000_images()\n",
    "            self.calculate_derived_metrics()\n",
    "            self.perform_statistical_analysis()\n",
    "            self.perform_machine_learning_analysis()\n",
    "            self.generate_comprehensive_visualizations()\n",
    "            self.generate_comprehensive_report()\n",
    "            self.save_results()\n",
    "            \n",
    "            # Print summary\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"üìä Images processed: {len(self.df):,}\")\n",
    "            print(f\"üß† Regions analyzed: {self.df['region'].nunique()}\")\n",
    "            print(f\"üë• Unique cases: {self.df['case_id'].nunique()}\")\n",
    "            \n",
    "            if 'ml' in self.results and self.results['ml'].get('results'):\n",
    "                best_accuracy = max([r['accuracy'] for r in self.results['ml']['results'].values()])\n",
    "                print(f\"ü§ñ Best ML accuracy: {best_accuracy:.3f}\")\n",
    "            \n",
    "            print(f\"üìà Mean SPPP: {self.df['sppp'].mean():.2f}%\")\n",
    "            \n",
    "            abc_counts = self.df['abc_level'].value_counts()\n",
    "            print(f\"üéØ ABC distribution:\")\n",
    "            for level, count in abc_counts.items():\n",
    "                print(f\"   - {level}: {count} images ({count/len(self.df)*100:.1f}%)\")\n",
    "            \n",
    "            print(f\"\\nüìÅ Results saved in: {self.results_dir}\")\n",
    "            print(f\"üìä Figures: {self.results_dir}/figures/\")\n",
    "            print(f\"üìã Tables: {self.results_dir}/tables/\")\n",
    "            print(f\"üìù Report: {self.results_dir}/comprehensive_analysis_report.md\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in analysis pipeline: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    output_dir = \"/nashome/bhavesh/bdsa-workflows-slurm/output\"\n",
    "    results_dir = \"/nashome/bhavesh/latest-workflow/bdsa-workflows-slurm/Digital-Neuropathology-Analysis-Framework/results\"\n",
    "    \n",
    "    # Initialize and run analysis\n",
    "    analyzer = LargeScaleNeuropathologyAnalysis(output_dir, results_dir)\n",
    "    analyzer.run_complete_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f98279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating supplementary visualizations...\n",
      "‚úÖ Supplementary visualizations created\n"
     ]
    }
   ],
   "source": [
    "# SUPPLEMENTARY VISUALIZATION CODE\n",
    "\n",
    "def create_supplementary_figures(analyzer):\n",
    "    \"\"\"Create supplementary figures for publication\"\"\"\n",
    "    fig_dir = analyzer.results_dir / \"figures\" / \"supplementary\"\n",
    "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df = analyzer.df\n",
    "    \n",
    "    # 1. Age distribution by ABC score\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=df, x='abc_level', y='age')\n",
    "    plt.title('Age Distribution by ABC Score')\n",
    "    plt.xlabel('ABC Score')\n",
    "    plt.ylabel('Age (years)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / 'age_by_abc.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. MMSE correlation scatterplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    valid_data = df.dropna(subset=['sppp', 'mmse'])\n",
    "    if len(valid_data) > 0:\n",
    "        plt.scatter(valid_data['sppp'], valid_data['mmse'], alpha=0.6, s=30)\n",
    "        plt.xlabel('SPPP (%)')\n",
    "        plt.ylabel('MMSE Score')\n",
    "        plt.title('Correlation: SPPP vs MMSE')\n",
    "        \n",
    "        # Add trendline if enough data points\n",
    "        if len(valid_data) > 2:\n",
    "            z = np.polyfit(valid_data['sppp'], valid_data['mmse'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(valid_data['sppp'], p(valid_data['sppp']), \"r--\", alpha=0.8)\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_dir / 'sppp_mmse_correlation.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. Processing time analysis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    valid_data = df.dropna(subset=['total_pixels', 'processing_time'])\n",
    "    if len(valid_data) > 0:\n",
    "        plt.scatter(valid_data['total_pixels'] / 1e6, valid_data['processing_time'], alpha=0.6, s=30)\n",
    "        plt.xlabel('Image Size (M pixels)')\n",
    "        plt.ylabel('Processing Time (s)')\n",
    "        plt.title('Processing Time vs Image Size')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    df['processing_efficiency'] = df['total_pixels'] / df['processing_time'].replace(0, 1) / 1e6\n",
    "    valid_data = df.dropna(subset=['region', 'processing_efficiency'])\n",
    "    if len(valid_data) > 0:\n",
    "        sns.boxplot(data=valid_data, x='region', y='processing_efficiency')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Processing Efficiency (M pixels/s)')\n",
    "        plt.title('Processing Efficiency by Region')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / 'processing_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Class distribution visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if 'abc_level' in df.columns:\n",
    "        abc_counts = df['abc_level'].value_counts()\n",
    "        colors = sns.color_palette(\"viridis\", len(abc_counts))\n",
    "        abc_counts.plot(kind='bar', color=colors)\n",
    "        plt.title('ABC Score Distribution')\n",
    "        plt.ylabel('Number of Images')\n",
    "        plt.xlabel('ABC Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_dir / 'abc_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 5. Regional pathology patterns\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    regional_stats = df.groupby('region').agg({\n",
    "        'sppp': 'mean',\n",
    "        'total_positivity': 'mean',\n",
    "        'braak_score': 'mean'\n",
    "    }).sort_values('sppp', ascending=False)\n",
    "    \n",
    "    if len(regional_stats) > 0:\n",
    "        sns.heatmap(regional_stats.T, annot=True, cmap='viridis', fmt='.2f',\n",
    "                   cbar_kws={'label': 'Mean Value'})\n",
    "        plt.title('Regional Pathology Patterns')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_dir / 'regional_patterns.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 6. ML performance comparison (if available)\n",
    "    if 'ml' in analyzer.results and analyzer.results['ml'].get('results'):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        model_names = []\n",
    "        accuracies = []\n",
    "        \n",
    "        for name, result in analyzer.results['ml']['results'].items():\n",
    "            model_names.append(name)\n",
    "            accuracies.append(result['accuracy'])\n",
    "        \n",
    "        colors = sns.color_palette(\"viridis\", len(model_names))\n",
    "        bars = plt.bar(model_names, accuracies, color=colors)\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Machine Learning Model Performance')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Add accuracy labels on bars\n",
    "        for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{acc:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_dir / 'ml_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# Run supplementary visualizations after the main analysis\n",
    "print(\"üìä Creating supplementary visualizations...\")\n",
    "create_supplementary_figures(analyzer)\n",
    "print(\"‚úÖ Supplementary visualizations created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
